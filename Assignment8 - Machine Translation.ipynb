{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDMOF-H41xp4"
   },
   "source": [
    "# Assignment NMT: Intro to NLP\n",
    "\n",
    "The aim of this homework is to familiarize you with sequence-to-sequence language modeling, specifically using an encoder-decoder model. In this notebook, you are provided with pre-written code for a simple sequence-to-sequence model that already works and learns how to reverse short sequences of numbers.\n",
    "\n",
    "If you run this whole jupyter notebook, it will learn to reverse short sequences of numbers. Although much of this code you will not be modifying, we recommend reading through it to get a sense of how the model and training works.\n",
    "\n",
    "This starter code is based on [this tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) by Sean Robertson from the PyTorch website and the COMS course at Columbia University by Professor Kathy McKeown. \n",
    "\n",
    "### Overview\n",
    "\n",
    "Your assignment is to:\n",
    " 1. adapt this notebook to work on the English-Italian language pair from Tataoeba website\n",
    " 2. Implement a beam search function\n",
    " \n",
    "\n",
    "Write all your code **in this jupyter notebook**. Cells are provided where you should be implementing your code. \n",
    "\n",
    "You do not need to modify any code to train the model. You may modify the `trainIters` function, if you would like to improve how you track progress, or change parameters while training. For example, it can be useful to decrease the teacher-forcing ratio as training progresses.\n",
    "\n",
    "\n",
    "I would recommend you run this notebook as it is, first. You should be able to run it with the dummy data provided without making ANY modifications (except the cell where the data are loaded). Then, start making your changes as requested.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cQESKYGw1xp6"
   },
   "outputs": [],
   "source": [
    "# You may modify this cell if you like\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1F6op_f1xp9",
    "outputId": "e2fcb2c5-e90d-414e-9d2c-aa2d003a048f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NO MODIFY\n",
    "\n",
    "# this is useful for checking if your code is successfully using the GPU\n",
    "\n",
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mydevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IbUybDZ91xp_"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cogsJCg51xqE"
   },
   "source": [
    "### Load dummy number reversal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3yyYL6esBZo",
    "outputId": "8e39a467-e8bd-466c-cca6-bba406ce648a"
   },
   "outputs": [],
   "source": [
    "#note that my files were stored in google drive and I was using google colab to run this notebook\n",
    "#you can change this cell to provide local path to load your training and dev data\n",
    "\n",
    "train_path = 'data/train/data.txt'\n",
    "dev_path = 'data/dev/data.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "An66dGkt1xqF"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "\n",
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3Z0rO701xqH"
   },
   "source": [
    "### 1. Load the data (10 points)\n",
    "\n",
    "Load in the en-it data from http://www.manythings.org/anki/ita-eng.zip\n",
    "\n",
    " similar to how the dummy number reversal dataset is loaded above. That is, use the same `torchtext.data.Field` and `torchtext.data.TabularDataset` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3zr76n7W1xqI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size: 341554\n",
      "ita_train len: 167360\n",
      "ita_val len: 71727\n",
      "ita_test len: 102467\n",
      "==================\n",
      "total: 341554\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE FOR LOADING THE ITALIAN<->ENGLISH DATA HERE\n",
    "#TODO \n",
    "#create 6 files, English|Italian data for train|dev|test, one sentence per line.\n",
    "#you can ignore this cell when you are running the code the first time through on the dummy reversal dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ita_path = \"data/anki/ita.txt\"\n",
    "\n",
    "ita_df = pd.read_csv(ita_path,sep='\\t', names=[\"English\", \"Italian\", \"Attribution\"]).drop([\"Attribution\"], axis=1)\n",
    "print(\"total size:\", len(ita_df))\n",
    "\n",
    "ita_train, ita_test = train_test_split(ita_df, test_size=0.3, shuffle=True, random_state=4111)\n",
    "\n",
    "ita_train, ita_val = train_test_split(ita_train, test_size=0.3, shuffle=True, random_state=4111)\n",
    "\n",
    "print(\"ita_train len:\", len(ita_train))\n",
    "print(\"ita_val len:\", len(ita_val))\n",
    "print(\"ita_test len:\", len(ita_test))\n",
    "print(\"==================\")\n",
    "print(\"total:\", len(ita_train) + len(ita_val) + len(ita_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ita_train_path = \"data/train/ita_train.tsv\"\n",
    "ita_val_path = \"data/dev/ita_dev.tsv\"\n",
    "\n",
    "ita_train.to_csv(ita_train_path, sep=\"\\t\", index=False, header=False)\n",
    "ita_val.to_csv(ita_val_path, sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=ita_train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=ita_val_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqisn6dd1xqK"
   },
   "source": [
    "Have a look at the vocab and some example data points.\n",
    "\n",
    "*If you have loaded in the data correctly, the code in the cell below should work without any modification.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yw7eK-hj1xqL",
    "outputId": "39c054f1-7573-4ad0-c2ee-1d95ed05a81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['<unk>', '<pad>', 'I', 'Tom', 'to', 'you', 'a', 'the', 'is', 'in', 'was', \"I'm\", 'of', 'have', 'You', 'that', 'do', 'be', 'He', 'for']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['<unk>', '<pad>', '<eos>', '<sos>', 'Tom', 'di', 'è', 'a', 'non', 'che', 'Io', 'Non', 'un', 'la', 'il', 'ha', 'per', 'in', 'sono', 'una']\n",
      "\n",
      "num training examples: 167358\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['How', 'many', 'friends', 'do', 'you', 'have?']\n",
      "tgt:\n",
      " ['<sos>', 'Quante', 'amiche', 'avete?', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell if you like\n",
    "\n",
    "src.build_vocab(data_train, max_size=50000)\n",
    "tgt.build_vocab(data_train, max_size=50000)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)\n",
    "\n",
    "#showing output below for toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7ZGeEix1xqN"
   },
   "source": [
    "### Model definition and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IDN_VycQ1xqO"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, myinput, hidden):\n",
    "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aA1Xp8Jb1xqR"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # zero the gradients of the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "    \n",
    "    # pass the inputs through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                \n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_fvIGMXl1xqT"
   },
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
    "    print(f'Running {n_iters} epochs...')\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "\n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # note batch size of 1, just for simplicity\n",
    "    # DO NOT INCREASE THE BATCH SIZE\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=1,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=mydevice, repeat=False)\n",
    "    \n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "            \n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "            \n",
    "            # this is because we're not actually using the batches.\n",
    "            # batch size is 1 and this just selects that first one\n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "            \n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
    "                start = time.time()\n",
    "        \n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CYrxT_n1xqV"
   },
   "source": [
    "###  Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bOBbHdMM1xqV"
   },
   "outputs": [],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "hidden_size = 128\n",
    "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
    "decoder1 = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCKVT2Y11xqX"
   },
   "source": [
    "Here are some guidelines for how much training to expect. Note that these *guidelines*; they are not exact.\n",
    "\n",
    "Only 1 epoch is needed for the number reversal dataset. This produces near-perfect results, and should take less than 5 minutes to run on a CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCAZ8hbk1xqY",
    "outputId": "9d040289-4d0a-438e-8eed-1cf561d30bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2 epochs...\n",
      "step: 1000\t avg loss: 5.80\t time for 1000 steps: 0.25 min\n",
      "step: 2000\t avg loss: 5.13\t time for 1000 steps: 0.24 min\n",
      "step: 3000\t avg loss: 4.91\t time for 1000 steps: 0.25 min\n",
      "step: 4000\t avg loss: 4.79\t time for 1000 steps: 0.25 min\n",
      "step: 5000\t avg loss: 4.70\t time for 1000 steps: 0.25 min\n",
      "step: 6000\t avg loss: 4.53\t time for 1000 steps: 0.26 min\n",
      "step: 7000\t avg loss: 4.51\t time for 1000 steps: 0.25 min\n",
      "step: 8000\t avg loss: 4.39\t time for 1000 steps: 0.25 min\n",
      "step: 9000\t avg loss: 4.48\t time for 1000 steps: 0.24 min\n",
      "step: 10000\t avg loss: 4.30\t time for 1000 steps: 0.22 min\n",
      "step: 11000\t avg loss: 4.30\t time for 1000 steps: 0.26 min\n",
      "step: 12000\t avg loss: 4.23\t time for 1000 steps: 0.25 min\n",
      "step: 13000\t avg loss: 4.23\t time for 1000 steps: 0.25 min\n",
      "step: 14000\t avg loss: 4.16\t time for 1000 steps: 0.26 min\n",
      "step: 15000\t avg loss: 4.06\t time for 1000 steps: 0.26 min\n",
      "step: 16000\t avg loss: 4.14\t time for 1000 steps: 0.26 min\n",
      "step: 17000\t avg loss: 4.05\t time for 1000 steps: 0.23 min\n",
      "step: 18000\t avg loss: 4.11\t time for 1000 steps: 0.25 min\n",
      "step: 19000\t avg loss: 4.06\t time for 1000 steps: 0.25 min\n",
      "step: 20000\t avg loss: 3.99\t time for 1000 steps: 0.25 min\n",
      "step: 21000\t avg loss: 3.98\t time for 1000 steps: 0.26 min\n",
      "step: 22000\t avg loss: 3.91\t time for 1000 steps: 0.24 min\n",
      "step: 23000\t avg loss: 3.92\t time for 1000 steps: 0.24 min\n",
      "step: 24000\t avg loss: 3.87\t time for 1000 steps: 0.26 min\n",
      "step: 25000\t avg loss: 3.89\t time for 1000 steps: 0.27 min\n",
      "step: 26000\t avg loss: 3.84\t time for 1000 steps: 0.27 min\n",
      "step: 27000\t avg loss: 3.81\t time for 1000 steps: 0.25 min\n",
      "step: 28000\t avg loss: 3.86\t time for 1000 steps: 0.26 min\n",
      "step: 29000\t avg loss: 3.85\t time for 1000 steps: 0.25 min\n",
      "step: 30000\t avg loss: 3.79\t time for 1000 steps: 0.24 min\n",
      "step: 31000\t avg loss: 3.89\t time for 1000 steps: 0.24 min\n",
      "step: 32000\t avg loss: 3.88\t time for 1000 steps: 0.25 min\n",
      "step: 33000\t avg loss: 3.84\t time for 1000 steps: 0.25 min\n",
      "step: 34000\t avg loss: 3.74\t time for 1000 steps: 0.26 min\n",
      "step: 35000\t avg loss: 3.70\t time for 1000 steps: 0.25 min\n",
      "step: 36000\t avg loss: 3.74\t time for 1000 steps: 0.26 min\n",
      "step: 37000\t avg loss: 3.75\t time for 1000 steps: 0.26 min\n",
      "step: 38000\t avg loss: 3.60\t time for 1000 steps: 0.24 min\n",
      "step: 39000\t avg loss: 3.69\t time for 1000 steps: 0.26 min\n",
      "step: 40000\t avg loss: 3.66\t time for 1000 steps: 0.26 min\n",
      "step: 41000\t avg loss: 3.62\t time for 1000 steps: 0.26 min\n",
      "step: 42000\t avg loss: 3.74\t time for 1000 steps: 0.26 min\n",
      "step: 43000\t avg loss: 3.64\t time for 1000 steps: 0.25 min\n",
      "step: 44000\t avg loss: 3.65\t time for 1000 steps: 0.25 min\n",
      "step: 45000\t avg loss: 3.56\t time for 1000 steps: 0.25 min\n",
      "step: 46000\t avg loss: 3.62\t time for 1000 steps: 0.25 min\n",
      "step: 47000\t avg loss: 3.57\t time for 1000 steps: 0.27 min\n",
      "step: 48000\t avg loss: 3.70\t time for 1000 steps: 0.26 min\n",
      "step: 49000\t avg loss: 3.61\t time for 1000 steps: 0.24 min\n",
      "step: 50000\t avg loss: 3.61\t time for 1000 steps: 0.25 min\n",
      "step: 51000\t avg loss: 3.55\t time for 1000 steps: 0.26 min\n",
      "step: 52000\t avg loss: 3.52\t time for 1000 steps: 0.26 min\n",
      "step: 53000\t avg loss: 3.54\t time for 1000 steps: 0.27 min\n",
      "step: 54000\t avg loss: 3.44\t time for 1000 steps: 0.25 min\n",
      "step: 55000\t avg loss: 3.44\t time for 1000 steps: 0.25 min\n",
      "step: 56000\t avg loss: 3.43\t time for 1000 steps: 0.25 min\n",
      "step: 57000\t avg loss: 3.50\t time for 1000 steps: 0.25 min\n",
      "step: 58000\t avg loss: 3.43\t time for 1000 steps: 0.26 min\n",
      "step: 59000\t avg loss: 3.48\t time for 1000 steps: 0.26 min\n",
      "step: 60000\t avg loss: 3.45\t time for 1000 steps: 0.25 min\n",
      "step: 61000\t avg loss: 3.37\t time for 1000 steps: 0.25 min\n",
      "step: 62000\t avg loss: 3.45\t time for 1000 steps: 0.25 min\n",
      "step: 63000\t avg loss: 3.56\t time for 1000 steps: 0.25 min\n",
      "step: 64000\t avg loss: 3.48\t time for 1000 steps: 0.26 min\n",
      "step: 65000\t avg loss: 3.48\t time for 1000 steps: 0.27 min\n",
      "step: 66000\t avg loss: 3.38\t time for 1000 steps: 0.27 min\n",
      "step: 67000\t avg loss: 3.45\t time for 1000 steps: 0.26 min\n",
      "step: 68000\t avg loss: 3.45\t time for 1000 steps: 0.25 min\n",
      "step: 69000\t avg loss: 3.40\t time for 1000 steps: 0.28 min\n",
      "step: 70000\t avg loss: 3.34\t time for 1000 steps: 0.26 min\n",
      "step: 71000\t avg loss: 3.44\t time for 1000 steps: 0.27 min\n",
      "step: 72000\t avg loss: 3.41\t time for 1000 steps: 0.27 min\n",
      "step: 73000\t avg loss: 3.35\t time for 1000 steps: 0.26 min\n",
      "step: 74000\t avg loss: 3.31\t time for 1000 steps: 0.26 min\n",
      "step: 75000\t avg loss: 3.28\t time for 1000 steps: 0.27 min\n",
      "step: 76000\t avg loss: 3.39\t time for 1000 steps: 0.27 min\n",
      "step: 77000\t avg loss: 3.32\t time for 1000 steps: 0.27 min\n",
      "step: 78000\t avg loss: 3.34\t time for 1000 steps: 0.27 min\n",
      "step: 79000\t avg loss: 3.30\t time for 1000 steps: 0.26 min\n",
      "step: 80000\t avg loss: 3.24\t time for 1000 steps: 0.26 min\n",
      "step: 81000\t avg loss: 3.25\t time for 1000 steps: 0.26 min\n",
      "step: 82000\t avg loss: 3.26\t time for 1000 steps: 0.25 min\n",
      "step: 83000\t avg loss: 3.27\t time for 1000 steps: 0.25 min\n",
      "step: 84000\t avg loss: 3.30\t time for 1000 steps: 0.25 min\n",
      "step: 85000\t avg loss: 3.28\t time for 1000 steps: 0.26 min\n",
      "step: 86000\t avg loss: 3.29\t time for 1000 steps: 0.26 min\n",
      "step: 87000\t avg loss: 3.25\t time for 1000 steps: 0.25 min\n",
      "step: 88000\t avg loss: 3.23\t time for 1000 steps: 0.25 min\n",
      "step: 89000\t avg loss: 3.29\t time for 1000 steps: 0.25 min\n",
      "step: 90000\t avg loss: 3.23\t time for 1000 steps: 0.25 min\n",
      "step: 91000\t avg loss: 3.21\t time for 1000 steps: 0.26 min\n",
      "step: 92000\t avg loss: 3.25\t time for 1000 steps: 0.25 min\n",
      "step: 93000\t avg loss: 3.21\t time for 1000 steps: 0.25 min\n",
      "step: 94000\t avg loss: 3.21\t time for 1000 steps: 0.25 min\n",
      "step: 95000\t avg loss: 3.20\t time for 1000 steps: 0.26 min\n",
      "step: 96000\t avg loss: 3.23\t time for 1000 steps: 0.25 min\n",
      "step: 97000\t avg loss: 3.14\t time for 1000 steps: 0.25 min\n",
      "step: 98000\t avg loss: 3.23\t time for 1000 steps: 0.25 min\n",
      "step: 99000\t avg loss: 3.13\t time for 1000 steps: 0.26 min\n",
      "step: 100000\t avg loss: 3.19\t time for 1000 steps: 0.27 min\n",
      "step: 101000\t avg loss: 3.19\t time for 1000 steps: 0.26 min\n",
      "step: 102000\t avg loss: 3.18\t time for 1000 steps: 0.26 min\n",
      "step: 103000\t avg loss: 3.21\t time for 1000 steps: 0.27 min\n",
      "step: 104000\t avg loss: 3.25\t time for 1000 steps: 0.25 min\n",
      "step: 105000\t avg loss: 3.14\t time for 1000 steps: 0.25 min\n",
      "step: 106000\t avg loss: 3.16\t time for 1000 steps: 0.26 min\n",
      "step: 107000\t avg loss: 3.21\t time for 1000 steps: 0.25 min\n",
      "step: 108000\t avg loss: 3.27\t time for 1000 steps: 0.25 min\n",
      "step: 109000\t avg loss: 3.17\t time for 1000 steps: 0.24 min\n",
      "step: 110000\t avg loss: 3.14\t time for 1000 steps: 0.25 min\n",
      "step: 111000\t avg loss: 3.15\t time for 1000 steps: 0.26 min\n",
      "step: 112000\t avg loss: 3.21\t time for 1000 steps: 0.27 min\n",
      "step: 113000\t avg loss: 3.15\t time for 1000 steps: 0.25 min\n",
      "step: 114000\t avg loss: 3.17\t time for 1000 steps: 0.25 min\n",
      "step: 115000\t avg loss: 3.05\t time for 1000 steps: 0.25 min\n",
      "step: 116000\t avg loss: 3.17\t time for 1000 steps: 0.25 min\n",
      "step: 117000\t avg loss: 3.16\t time for 1000 steps: 0.25 min\n",
      "step: 118000\t avg loss: 3.13\t time for 1000 steps: 0.24 min\n",
      "step: 119000\t avg loss: 3.16\t time for 1000 steps: 0.26 min\n",
      "step: 120000\t avg loss: 3.20\t time for 1000 steps: 0.26 min\n",
      "step: 121000\t avg loss: 3.10\t time for 1000 steps: 0.26 min\n",
      "step: 122000\t avg loss: 3.10\t time for 1000 steps: 0.25 min\n",
      "step: 123000\t avg loss: 3.14\t time for 1000 steps: 0.25 min\n",
      "step: 124000\t avg loss: 3.12\t time for 1000 steps: 0.25 min\n",
      "step: 125000\t avg loss: 3.09\t time for 1000 steps: 0.26 min\n",
      "step: 126000\t avg loss: 3.06\t time for 1000 steps: 0.26 min\n",
      "step: 127000\t avg loss: 3.08\t time for 1000 steps: 0.27 min\n",
      "step: 128000\t avg loss: 3.03\t time for 1000 steps: 0.27 min\n",
      "step: 129000\t avg loss: 3.13\t time for 1000 steps: 0.27 min\n",
      "step: 130000\t avg loss: 3.10\t time for 1000 steps: 0.26 min\n",
      "step: 131000\t avg loss: 3.10\t time for 1000 steps: 0.27 min\n",
      "step: 132000\t avg loss: 2.99\t time for 1000 steps: 0.28 min\n",
      "step: 133000\t avg loss: 3.05\t time for 1000 steps: 0.26 min\n",
      "step: 134000\t avg loss: 3.05\t time for 1000 steps: 0.25 min\n",
      "step: 135000\t avg loss: 3.03\t time for 1000 steps: 0.25 min\n",
      "step: 136000\t avg loss: 3.07\t time for 1000 steps: 0.25 min\n",
      "step: 137000\t avg loss: 3.06\t time for 1000 steps: 0.25 min\n",
      "step: 138000\t avg loss: 3.06\t time for 1000 steps: 0.25 min\n",
      "step: 139000\t avg loss: 3.12\t time for 1000 steps: 0.27 min\n",
      "step: 140000\t avg loss: 3.12\t time for 1000 steps: 0.27 min\n",
      "step: 141000\t avg loss: 3.04\t time for 1000 steps: 0.26 min\n",
      "step: 142000\t avg loss: 3.06\t time for 1000 steps: 0.26 min\n",
      "step: 143000\t avg loss: 3.00\t time for 1000 steps: 0.26 min\n",
      "step: 144000\t avg loss: 2.95\t time for 1000 steps: 0.27 min\n",
      "step: 145000\t avg loss: 2.99\t time for 1000 steps: 0.27 min\n",
      "step: 146000\t avg loss: 3.07\t time for 1000 steps: 0.26 min\n",
      "step: 147000\t avg loss: 2.97\t time for 1000 steps: 0.26 min\n",
      "step: 148000\t avg loss: 2.97\t time for 1000 steps: 0.26 min\n",
      "step: 149000\t avg loss: 2.99\t time for 1000 steps: 0.26 min\n",
      "step: 150000\t avg loss: 2.97\t time for 1000 steps: 0.25 min\n",
      "step: 151000\t avg loss: 3.00\t time for 1000 steps: 0.25 min\n",
      "step: 152000\t avg loss: 2.99\t time for 1000 steps: 0.25 min\n",
      "step: 153000\t avg loss: 2.95\t time for 1000 steps: 0.25 min\n",
      "step: 154000\t avg loss: 2.98\t time for 1000 steps: 0.26 min\n",
      "step: 155000\t avg loss: 3.02\t time for 1000 steps: 0.28 min\n",
      "step: 156000\t avg loss: 3.00\t time for 1000 steps: 0.25 min\n",
      "step: 157000\t avg loss: 3.02\t time for 1000 steps: 0.26 min\n",
      "step: 158000\t avg loss: 2.94\t time for 1000 steps: 0.25 min\n",
      "step: 159000\t avg loss: 3.04\t time for 1000 steps: 0.26 min\n",
      "step: 160000\t avg loss: 2.92\t time for 1000 steps: 0.26 min\n",
      "step: 161000\t avg loss: 2.97\t time for 1000 steps: 0.25 min\n",
      "step: 162000\t avg loss: 2.90\t time for 1000 steps: 0.24 min\n",
      "step: 163000\t avg loss: 2.98\t time for 1000 steps: 0.25 min\n",
      "step: 164000\t avg loss: 2.89\t time for 1000 steps: 0.24 min\n",
      "step: 165000\t avg loss: 2.93\t time for 1000 steps: 0.25 min\n",
      "step: 166000\t avg loss: 2.96\t time for 1000 steps: 0.27 min\n",
      "step: 167000\t avg loss: 2.92\t time for 1000 steps: 0.26 min\n",
      "End of epoch 0, avg loss 3.44\n",
      "step: 1000\t avg loss: 3.66\t time for 1000 steps: 0.26 min\n",
      "step: 2000\t avg loss: 2.72\t time for 1000 steps: 0.26 min\n",
      "step: 3000\t avg loss: 2.82\t time for 1000 steps: 0.27 min\n",
      "step: 4000\t avg loss: 2.82\t time for 1000 steps: 0.26 min\n",
      "step: 5000\t avg loss: 2.75\t time for 1000 steps: 0.25 min\n",
      "step: 6000\t avg loss: 2.78\t time for 1000 steps: 0.26 min\n",
      "step: 7000\t avg loss: 2.73\t time for 1000 steps: 0.26 min\n",
      "step: 8000\t avg loss: 2.80\t time for 1000 steps: 0.26 min\n",
      "step: 9000\t avg loss: 2.77\t time for 1000 steps: 0.26 min\n",
      "step: 10000\t avg loss: 2.76\t time for 1000 steps: 0.27 min\n",
      "step: 11000\t avg loss: 2.77\t time for 1000 steps: 0.25 min\n",
      "step: 12000\t avg loss: 2.79\t time for 1000 steps: 0.26 min\n",
      "step: 13000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 14000\t avg loss: 2.80\t time for 1000 steps: 0.27 min\n",
      "step: 15000\t avg loss: 2.74\t time for 1000 steps: 0.26 min\n",
      "step: 16000\t avg loss: 2.75\t time for 1000 steps: 0.27 min\n",
      "step: 17000\t avg loss: 2.73\t time for 1000 steps: 0.27 min\n",
      "step: 18000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 19000\t avg loss: 2.74\t time for 1000 steps: 0.27 min\n",
      "step: 20000\t avg loss: 2.71\t time for 1000 steps: 0.25 min\n",
      "step: 21000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 22000\t avg loss: 2.70\t time for 1000 steps: 0.26 min\n",
      "step: 23000\t avg loss: 2.71\t time for 1000 steps: 0.26 min\n",
      "step: 24000\t avg loss: 2.75\t time for 1000 steps: 0.26 min\n",
      "step: 25000\t avg loss: 2.80\t time for 1000 steps: 0.25 min\n",
      "step: 26000\t avg loss: 2.76\t time for 1000 steps: 0.25 min\n",
      "step: 27000\t avg loss: 2.70\t time for 1000 steps: 0.27 min\n",
      "step: 28000\t avg loss: 2.72\t time for 1000 steps: 0.25 min\n",
      "step: 29000\t avg loss: 2.77\t time for 1000 steps: 0.25 min\n",
      "step: 30000\t avg loss: 2.79\t time for 1000 steps: 0.25 min\n",
      "step: 31000\t avg loss: 2.65\t time for 1000 steps: 0.25 min\n",
      "step: 32000\t avg loss: 2.67\t time for 1000 steps: 0.25 min\n",
      "step: 33000\t avg loss: 2.66\t time for 1000 steps: 0.26 min\n",
      "step: 34000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 35000\t avg loss: 2.72\t time for 1000 steps: 0.25 min\n",
      "step: 36000\t avg loss: 2.75\t time for 1000 steps: 0.25 min\n",
      "step: 37000\t avg loss: 2.69\t time for 1000 steps: 0.25 min\n",
      "step: 38000\t avg loss: 2.79\t time for 1000 steps: 0.26 min\n",
      "step: 39000\t avg loss: 2.77\t time for 1000 steps: 0.26 min\n",
      "step: 40000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 41000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 42000\t avg loss: 2.72\t time for 1000 steps: 0.25 min\n",
      "step: 43000\t avg loss: 2.77\t time for 1000 steps: 0.25 min\n",
      "step: 44000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 45000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 46000\t avg loss: 2.70\t time for 1000 steps: 0.25 min\n",
      "step: 47000\t avg loss: 2.71\t time for 1000 steps: 0.25 min\n",
      "step: 48000\t avg loss: 2.74\t time for 1000 steps: 0.25 min\n",
      "step: 49000\t avg loss: 2.66\t time for 1000 steps: 0.26 min\n",
      "step: 50000\t avg loss: 2.71\t time for 1000 steps: 0.26 min\n",
      "step: 51000\t avg loss: 2.70\t time for 1000 steps: 0.26 min\n",
      "step: 52000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 53000\t avg loss: 2.63\t time for 1000 steps: 0.25 min\n",
      "step: 54000\t avg loss: 2.75\t time for 1000 steps: 0.26 min\n",
      "step: 55000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 56000\t avg loss: 2.78\t time for 1000 steps: 0.25 min\n",
      "step: 57000\t avg loss: 2.77\t time for 1000 steps: 0.25 min\n",
      "step: 58000\t avg loss: 2.74\t time for 1000 steps: 0.25 min\n",
      "step: 59000\t avg loss: 2.74\t time for 1000 steps: 0.26 min\n",
      "step: 60000\t avg loss: 2.73\t time for 1000 steps: 0.28 min\n",
      "step: 61000\t avg loss: 2.68\t time for 1000 steps: 0.27 min\n",
      "step: 62000\t avg loss: 2.75\t time for 1000 steps: 0.25 min\n",
      "step: 63000\t avg loss: 2.71\t time for 1000 steps: 0.27 min\n",
      "step: 64000\t avg loss: 2.67\t time for 1000 steps: 0.27 min\n",
      "step: 65000\t avg loss: 2.67\t time for 1000 steps: 0.25 min\n",
      "step: 66000\t avg loss: 2.71\t time for 1000 steps: 0.26 min\n",
      "step: 67000\t avg loss: 2.63\t time for 1000 steps: 0.25 min\n",
      "step: 68000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 69000\t avg loss: 2.75\t time for 1000 steps: 0.25 min\n",
      "step: 70000\t avg loss: 2.75\t time for 1000 steps: 0.25 min\n",
      "step: 71000\t avg loss: 2.66\t time for 1000 steps: 0.25 min\n",
      "step: 72000\t avg loss: 2.71\t time for 1000 steps: 0.25 min\n",
      "step: 73000\t avg loss: 2.70\t time for 1000 steps: 0.25 min\n",
      "step: 74000\t avg loss: 2.65\t time for 1000 steps: 0.24 min\n",
      "step: 75000\t avg loss: 2.75\t time for 1000 steps: 0.26 min\n",
      "step: 76000\t avg loss: 2.67\t time for 1000 steps: 0.26 min\n",
      "step: 77000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 78000\t avg loss: 2.74\t time for 1000 steps: 0.26 min\n",
      "step: 79000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 80000\t avg loss: 2.67\t time for 1000 steps: 0.26 min\n",
      "step: 81000\t avg loss: 2.66\t time for 1000 steps: 0.25 min\n",
      "step: 82000\t avg loss: 2.76\t time for 1000 steps: 0.25 min\n",
      "step: 83000\t avg loss: 2.66\t time for 1000 steps: 0.25 min\n",
      "step: 84000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 85000\t avg loss: 2.74\t time for 1000 steps: 0.26 min\n",
      "step: 86000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 87000\t avg loss: 2.61\t time for 1000 steps: 0.26 min\n",
      "step: 88000\t avg loss: 2.63\t time for 1000 steps: 0.26 min\n",
      "step: 89000\t avg loss: 2.67\t time for 1000 steps: 0.26 min\n",
      "step: 90000\t avg loss: 2.73\t time for 1000 steps: 0.26 min\n",
      "step: 91000\t avg loss: 2.80\t time for 1000 steps: 0.25 min\n",
      "step: 92000\t avg loss: 2.73\t time for 1000 steps: 0.25 min\n",
      "step: 93000\t avg loss: 2.69\t time for 1000 steps: 0.25 min\n",
      "step: 94000\t avg loss: 2.69\t time for 1000 steps: 0.24 min\n",
      "step: 95000\t avg loss: 2.67\t time for 1000 steps: 0.26 min\n",
      "step: 96000\t avg loss: 2.70\t time for 1000 steps: 0.26 min\n",
      "step: 97000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 98000\t avg loss: 2.65\t time for 1000 steps: 0.25 min\n",
      "step: 99000\t avg loss: 2.67\t time for 1000 steps: 0.25 min\n",
      "step: 100000\t avg loss: 2.59\t time for 1000 steps: 0.25 min\n",
      "step: 101000\t avg loss: 2.67\t time for 1000 steps: 0.25 min\n",
      "step: 102000\t avg loss: 2.55\t time for 1000 steps: 0.25 min\n",
      "step: 103000\t avg loss: 2.67\t time for 1000 steps: 0.25 min\n",
      "step: 104000\t avg loss: 2.79\t time for 1000 steps: 0.27 min\n",
      "step: 105000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 106000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 107000\t avg loss: 2.66\t time for 1000 steps: 0.26 min\n",
      "step: 108000\t avg loss: 2.66\t time for 1000 steps: 0.25 min\n",
      "step: 109000\t avg loss: 2.66\t time for 1000 steps: 0.25 min\n",
      "step: 110000\t avg loss: 2.69\t time for 1000 steps: 0.25 min\n",
      "step: 111000\t avg loss: 2.72\t time for 1000 steps: 0.26 min\n",
      "step: 112000\t avg loss: 2.71\t time for 1000 steps: 0.26 min\n",
      "step: 113000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 114000\t avg loss: 2.64\t time for 1000 steps: 0.25 min\n",
      "step: 115000\t avg loss: 2.65\t time for 1000 steps: 0.26 min\n",
      "step: 116000\t avg loss: 2.58\t time for 1000 steps: 0.25 min\n",
      "step: 117000\t avg loss: 2.68\t time for 1000 steps: 0.27 min\n",
      "step: 118000\t avg loss: 2.62\t time for 1000 steps: 0.26 min\n",
      "step: 119000\t avg loss: 2.70\t time for 1000 steps: 0.25 min\n",
      "step: 120000\t avg loss: 2.59\t time for 1000 steps: 0.25 min\n",
      "step: 121000\t avg loss: 2.59\t time for 1000 steps: 0.27 min\n",
      "step: 122000\t avg loss: 2.65\t time for 1000 steps: 0.26 min\n",
      "step: 123000\t avg loss: 2.59\t time for 1000 steps: 0.27 min\n",
      "step: 124000\t avg loss: 2.61\t time for 1000 steps: 0.28 min\n",
      "step: 125000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 126000\t avg loss: 2.59\t time for 1000 steps: 0.25 min\n",
      "step: 127000\t avg loss: 2.55\t time for 1000 steps: 0.25 min\n",
      "step: 128000\t avg loss: 2.67\t time for 1000 steps: 0.26 min\n",
      "step: 129000\t avg loss: 2.69\t time for 1000 steps: 0.26 min\n",
      "step: 130000\t avg loss: 2.64\t time for 1000 steps: 0.27 min\n",
      "step: 131000\t avg loss: 2.62\t time for 1000 steps: 0.26 min\n",
      "step: 132000\t avg loss: 2.68\t time for 1000 steps: 0.26 min\n",
      "step: 133000\t avg loss: 2.65\t time for 1000 steps: 0.25 min\n",
      "step: 134000\t avg loss: 2.68\t time for 1000 steps: 0.25 min\n",
      "step: 135000\t avg loss: 2.63\t time for 1000 steps: 0.27 min\n",
      "step: 136000\t avg loss: 2.71\t time for 1000 steps: 0.26 min\n",
      "step: 137000\t avg loss: 2.55\t time for 1000 steps: 0.26 min\n",
      "step: 138000\t avg loss: 2.59\t time for 1000 steps: 0.27 min\n",
      "step: 139000\t avg loss: 2.75\t time for 1000 steps: 0.26 min\n",
      "step: 140000\t avg loss: 2.61\t time for 1000 steps: 0.25 min\n",
      "step: 141000\t avg loss: 2.51\t time for 1000 steps: 0.25 min\n",
      "step: 142000\t avg loss: 2.58\t time for 1000 steps: 0.25 min\n",
      "step: 143000\t avg loss: 2.62\t time for 1000 steps: 0.25 min\n",
      "step: 144000\t avg loss: 2.65\t time for 1000 steps: 0.26 min\n",
      "step: 145000\t avg loss: 2.62\t time for 1000 steps: 0.27 min\n",
      "step: 146000\t avg loss: 2.53\t time for 1000 steps: 0.26 min\n",
      "step: 147000\t avg loss: 2.58\t time for 1000 steps: 0.26 min\n",
      "step: 148000\t avg loss: 2.60\t time for 1000 steps: 0.26 min\n",
      "step: 149000\t avg loss: 2.65\t time for 1000 steps: 0.26 min\n",
      "step: 150000\t avg loss: 2.55\t time for 1000 steps: 0.27 min\n",
      "step: 151000\t avg loss: 2.53\t time for 1000 steps: 0.26 min\n",
      "step: 152000\t avg loss: 2.61\t time for 1000 steps: 0.26 min\n",
      "step: 153000\t avg loss: 2.57\t time for 1000 steps: 0.25 min\n",
      "step: 154000\t avg loss: 2.59\t time for 1000 steps: 0.25 min\n",
      "step: 155000\t avg loss: 2.62\t time for 1000 steps: 0.27 min\n",
      "step: 156000\t avg loss: 2.54\t time for 1000 steps: 0.25 min\n",
      "step: 157000\t avg loss: 2.61\t time for 1000 steps: 0.25 min\n",
      "step: 158000\t avg loss: 2.66\t time for 1000 steps: 0.25 min\n",
      "step: 159000\t avg loss: 2.62\t time for 1000 steps: 0.26 min\n",
      "step: 160000\t avg loss: 2.51\t time for 1000 steps: 0.25 min\n",
      "step: 161000\t avg loss: 2.60\t time for 1000 steps: 0.26 min\n",
      "step: 162000\t avg loss: 2.57\t time for 1000 steps: 0.26 min\n",
      "step: 163000\t avg loss: 2.59\t time for 1000 steps: 0.27 min\n",
      "step: 164000\t avg loss: 2.56\t time for 1000 steps: 0.27 min\n",
      "step: 165000\t avg loss: 2.57\t time for 1000 steps: 0.26 min\n",
      "step: 166000\t avg loss: 2.67\t time for 1000 steps: 0.26 min\n",
      "step: 167000\t avg loss: 2.62\t time for 1000 steps: 0.25 min\n",
      "End of epoch 1, avg loss 2.68\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "# but be sure that it prints some indication of how training is progressing\n",
    "\n",
    "trainIters(encoder1, decoder1, 2, print_every=1000, learning_rate=0.05, teacher_forcing_ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "m2G_5Uwx1xqg"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            next_word = output_vocab.itos[topi.item()]\n",
    "            decoded_words.append(next_word)\n",
    "            if next_word == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDG-Sydz1xql"
   },
   "source": [
    "Have a look at some generated sequences! This is the fun part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWPksDgH1xqm",
    "outputId": "19ab0572-2a2c-4aa7-be91-88fdf1084386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Prepare', 'to', 'evacuate.']\n",
      "<sos> Si a scrivere a dormire. <eos>\n",
      "\n",
      "[\"I'll\", 'go.']\n",
      "<sos> La andare. <eos>\n",
      "\n",
      "['Tom', \"wouldn't\", 'do', 'it.']\n",
      "<sos> Tom non lo odia. oggi. <eos>\n",
      "\n",
      "['Close', 'the', 'drawer.']\n",
      "<sos> Chiudi il piano. <eos>\n",
      "\n",
      "['I', 'hear', 'the', 'grass', 'in', 'England', 'is', 'green', 'even', 'in', 'the', 'winter.']\n",
      "<sos> Io ho sentito dei biblioteca al conto. di più. la terra. di storia. la terra. di storia. la terra. di storia. la terra. di storia. la terra. di storia. la terra. di storia. la terra. di storia. la terra. di più. la terra. di più. la terra. di più.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You may modify this cell\n",
    "\n",
    "# This selects 5 random datapoints from the training data and shows the generated sequence\n",
    "\n",
    "for i in range(5):\n",
    "    item = random.choice(data_train.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    print(' '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-owLs3W1xqn"
   },
   "source": [
    "### Implement beam search (10 points for ITCS 4111 students, 15 points for the ITCS 5111/DSBA 6010 students)\n",
    "\n",
    "We provide an evaluation function that performs greedy decoding on any input sequence provided. \n",
    "\n",
    "(ITCS 4111 students) You must write a new  function that implements beam search for a beam size = 2. \n",
    "\n",
    "(ITCS 5111/DSBA 6010 students) You must write a new  function that implements beam search for an arbitrary beam size. Let the beam size be an input parameter to your function. \n",
    "\n",
    "In greedy decoding, at each decoding step the most likely word is selected, resulting in one decoded sequence output. \n",
    "\n",
    "In beam search, at each decoding step the top k most\n",
    "likely sequences are selected. Each of these k sequences is then used to generate the next step; the top k next words per sequence are considered (for a total of k ∗ k sequences)\n",
    "and the top k sequences are selected to take to the next decoding step. At the end, you have k decoded sequence outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RM53b6XE1xqo"
   },
   "outputs": [],
   "source": [
    "# credit for original source of code is https://github.com/budzianowski/PyTorch-Beam-Search-Decoding/blob/master/decode_beam.py\n",
    "# I modified their algorithm and optimised it a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode:\n",
    "    def __init__(self, parent_node, hidden_state, word_id, score):\n",
    "        self.parent = parent_node\n",
    "        self.h = hidden_state\n",
    "        self.word_id = word_id\n",
    "        self.score = score\n",
    "        \n",
    "        if self.parent == None:\n",
    "            self.length = 1\n",
    "        else:\n",
    "            self.length = len(self.parent) +1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        ## Add here a function for shaping a reward\n",
    "\n",
    "        return self.score / float(self.length - 1 + 1e-6) + alpha * reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from queue import PriorityQueue\n",
    "\n",
    "def beam_decode(encoder, decoder, sentence, max_length=MAX_LEN, width=2):\n",
    "    input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "    input_length = input_tensor.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice) #input is Start of Sentence token\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    endnodes = []\n",
    "\n",
    "    for idx in range(input_tensor.size(0)):\n",
    "        #print(idx)\n",
    "        #print(decoder_input)\n",
    "        \n",
    "        node = BeamSearchNode(None, decoder_hidden, decoder_input, 0)\n",
    "        nodes = PriorityQueue()\n",
    "        \n",
    "        nodes.put((-node.eval(), node))\n",
    "                \n",
    "        while True:\n",
    "            if nodes.qsize() > 2000: break\n",
    "            \n",
    "            #get current best node\n",
    "            score, current_node = nodes.get()\n",
    "            decoder_input = current_node.word_id\n",
    "            decoder_hidden = current_node.h\n",
    "            \n",
    "            if current_node.word_id.item() == EOS_TOKEN and current_node.parent != None:\n",
    "                endnodes.append((score, current_node))\n",
    "                break\n",
    "                \n",
    "            decoder_output, decoder_hidden = decoder1(decoder_input, decoder_hidden)\n",
    "            \n",
    "            log_probability, indexes = torch.topk(decoder_output, width)\n",
    "            \n",
    "            for k in range(width):\n",
    "                decoded_t = indexes[0][k].view(1, -1)\n",
    "                log_p = log_probability[0][k].item()\n",
    "                \n",
    "                node = BeamSearchNode(current_node, decoder_hidden, decoded_t, current_node.score + log_p)\n",
    "                score = -node.eval()\n",
    "                nodes.put((score, node))                \n",
    "        \n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(1)]\n",
    "\n",
    "        decoded_ids = []\n",
    "        sentence = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "\n",
    "            decoded_ids.append(n.word_id)\n",
    "            # back trace\n",
    "            while n.parent != None:\n",
    "                n = n.parent\n",
    "                decoded_ids.append(n.word_id)\n",
    "\n",
    "            decoded_ids = decoded_ids[::-1]\n",
    "            \n",
    "    for decoded_id in decoded_ids:\n",
    "        word = output_vocab.itos[decoded_id.item()]\n",
    "        if word != SOS_TOKEN and word != EOS_TOKEN:\n",
    "            sentence.append(word)            \n",
    "    return sentence\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Let Tom answer.\n",
      "Beam Search Translation: Lasciate Tom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "item = random.choice(data_train.examples)\n",
    "seq = item.src\n",
    "print(\"English:\",' '.join(seq))\n",
    "words = beam_decode(encoder1, decoder1, seq, width=3)\n",
    "print(\"Beam Search Translation:\",' '.join(words))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nmt_assignment_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
